{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "## This notebook contains:\n",
    "* Running Llama2 in the cloud hosted on Replicate\n",
    "* Using LangChain to ask Llama general questions and follow up questions\n",
    "* Using LangChain to load PDF docs - (Kenya Law Documents) - and chat about it.\n",
    "* The end result will be a chatbot that will be able answer questions about the data not publicly available when Llama2 was trained, or about your own data. RAG is one way to prevent LLM's hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- chromadb gives us database capabilities \n",
    "- langchain provides necessary RAG tools for this demo\n",
    "\n",
    "And setting up the Replicate token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c608df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain replicate sentence-transformers chromadb pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the Llama 2 model from replicate. In this example we will use the llama 2 13b chat model. You can find more Llama 2 models by searching for them on the [Replicate model explore page](https://replicate.com/explore?query=llama).\n",
    "\n",
    "The model is added in the format: model_name/version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb042eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Replicate API token...\n",
      "Replicate API token set.\n",
      "Initializing Llama2 model...\n",
      "Llama2 model initialized.\n"
     ]
    }
   ],
   "source": [
    "# set up the Replicate API token\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "print(\"Getting Replicate API token...\")\n",
    "REPLICATE_API_TOKEN = \"r8_aARpJdbqdFixYHVQ9LvCvaoRA0Svg8j2Pw5W3\" \n",
    "os.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN\n",
    "print(\"Replicate API token set.\")\n",
    "\n",
    "# initialize the Llama2 model\n",
    "\n",
    "from langchain.llms import Replicate\n",
    "\n",
    "print(\"Initializing Llama2 model...\")\n",
    "llama2_13b = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"\n",
    "llm = Replicate(\n",
    "    model=llama2_13b,\n",
    "    model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\":500}\n",
    ")\n",
    "print(\"Llama2 model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, it is now possible to ask some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493a7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'd be happy to help you with your question. The book \"The Innovator's Dilemma\" was written by Clayton Christensen, an American author and academic who is known for his work on innovation and disruptive technologies. He is a professor at Harvard Business School and has written several other influential books on business and leadership. Is there anything else you would like to know about this topic?\n"
     ]
    }
   ],
   "source": [
    "question = \"who wrote the book Innovator's dilemma?\"\n",
    "answer = llm.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315f000",
   "metadata": {},
   "source": [
    "We will then try to follow up the response with a question asking for more information on the book. \n",
    "\n",
    "Since the chat history is not passed on Llama doesn't have the context and doesn't know this is more about the book thus it treats this as new query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! As a helpful assistant, I'm here to assist you with any questions or tasks you may have. Whether it's providing information on a wide range of topics, helping you with a project or task, or simply being a listening ear, I'm here to help in any way I can.\n",
      "\n",
      "I have access to a vast amount of knowledge and resources, so if there's something you're curious about or need help with, feel free to ask! Some examples of things I can help with include:\n",
      "\n",
      "* Answering questions on a variety of topics such as history, science, technology, health, and more\n",
      "* Providing guidance on how to complete a task or project\n",
      "* Offering suggestions and ideas for creative projects or brainstorming sessions\n",
      "* Assisting with language translation and communication\n",
      "* And much more!\n",
      "\n",
      "Is there anything specific you would like to know or discuss? Please don't hesitate to ask, and I'll do my best to assist you.\n"
     ]
    }
   ],
   "source": [
    "# chat history not passed so Llama doesn't have the context and doesn't know this is more about the book\n",
    "followup = \"tell me more\"\n",
    "followup_answer = llm(followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeaffc7",
   "metadata": {},
   "source": [
    "[`ConversationBufferMemory`](https://python.langchain.com/docs/modules/memory/types/buffer) is used to pass the chat history to the model and give it the capability to handle follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5428ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using ConversationBufferMemory to pass memory (chat history) for follow up questions\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9af5f",
   "metadata": {},
   "source": [
    "Once this is set up, let us repeat the steps from before and ask the model a simple question.\n",
    "\n",
    "Then we pass the question and answer back into the model for context along with the follow up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baee2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ah, you're asking about \"The Innovator's Dilemma,\" the iconic book written by Clayton Christensen! He is a renowned author, professor, and business consultant known for his groundbreaking work in the fields of innovation and disruptive technologies.\n",
      "\n",
      "Published in 1997, \"The Innovator's Dilemma\" explores why successful companies often struggle to adapt to new technologies and business models that ultimately disrupt their industries. The book introduces the concept of \"disruptive innovation,\" which describes how small, unassuming ideas can eventually upend entire markets and leave established players behind.\n",
      "\n",
      "Clayton Christensen has since become one of the most influential voices in the business world, and his ideas have been applied across various sectors, from technology and healthcare to finance and education. His follow-up books, such as \"The Innovator's Solution\" and \"Competing Against Luck,\" further expand on these concepts and offer practical guidance for leaders looking to stay ahead of the curve.\n",
      "\n",
      "So there you have it â€“ the brilliant mind behind \"The Innovator's Dilemma\" is none other than Clayton Christensen!\n"
     ]
    }
   ],
   "source": [
    "# restart from the original question\n",
    "answer = conversation.predict(input=question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass context (previous question and answer) along with the follow up \"tell me more\" to Llama who now knows more of what\n",
    "memory.save_context({\"input\": question},\n",
    "                    {\"output\": answer})\n",
    "followup_answer = conversation.predict(input=followup)\n",
    "print(followup_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99516a1a",
   "metadata": {},
   "source": [
    "Load the documents from the data path\n",
    "\n",
    "Using Llama 2 to answer questions using documents for context. \n",
    "This gives us the ability to update Llama 2's knowledge thus giving it better context without needing to fine-tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c61ceb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "DATA_PATH = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f79ad922",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2cf3f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF files: 3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the number of PDF files in the data directory\n",
    "pdf_files = [f for f in os.listdir(DATA_PATH) if f.endswith('.pdf')]\n",
    "print(f\"Number of PDF files: {len(pdf_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "Storing the documents. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "In this case [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) is used which is light-weight and in memory so it's easy to get started with.\n",
    "\n",
    "We will also import the HuggingFaceEmbeddings and RecursiveCharacterTextSplitter to assist in storing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eecb6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# embeddings are numerical representations of the question and answer text\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# use a common text splitter to split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using [`HuggingFaceEmbeddings`](https://www.google.com/search?q=langchain+hugging+face+embeddings&sca_esv=572890011&ei=ARUoZaH4LuumptQP48ah2Ac&oq=langchian+hugg&gs_lp=Egxnd3Mtd2l6LXNlcnAiDmxhbmdjaGlhbiBodWdnKgIIADIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkjeHlC5Cli5D3ABeAGQAQCYAV6gAb4CqgEBNLgBAcgBAPgBAcICChAAGEcY1gQYsAPiAwQYACBBiAYBkAYI&sclient=gws-wiz-serp) on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc65e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector db to store all the split chunks as embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad02d7",
   "metadata": {},
   "source": [
    "We then use ` RetrievalQA` to retrieve the documents from the vector database and give the model more context on Llama 2, thereby increasing its knowledge.\n",
    "\n",
    "For each question, LangChain performs a semantic similarity search of it in the vector db, then passes the search results as the context to Llama to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00e3f72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context provided, I can provide information about the Obi Van Passi Government Marriage Act of 2025. This act was enacted to regulate marriage and procreation in order to promote population growth and stability within the nation.\n",
      "\n",
      "Here are some key points from the act:\n",
      "\n",
      "1. Minimum number of children: Each adult male citizen is required to father a minimum of ten children during their lifetime. Failure to meet this requirement will result in penalties.\n",
      "2. Open marriages: Open marriages, where spouses are permitted to engage in romantic or sexual relationships with individuals outside of their marriage, are legal and permissible for adult males. Female spouses are not permitted to engage in open marriages.\n",
      "3. Procreation obligations: The government shall monitor the procreation obligations outlined in Section 3 of the act, and citizens shall be required to report the birth of each child to the government within thirty days of the child's birth. Failure to report the birth of a child within the specified timeframe shall result in penalties.\n",
      "4. Penalties for non-compliance: Failure to meet the minimum procreation obligations outlined in Section 3 of the act shall result in penalties as outlined in Section 6 of the act.\n",
      "5. Exemptions: Individuals who are medically unable to meet the procreation obligations outlined in Section 3 of the act shall be exempt from such requirements. Exemptions may be granted by the government on a case-by-case basis, subject to verification by medical professionals.\n",
      "6. Effective date: This act shall take effect on the date of its enactment.\n",
      "7. Amendments: Amendments to this act may be made through the regular legislative process.\n",
      "8. Interpretation: This act shall be construed in accordance with the laws and constitution of the nation.\n",
      "9. Citation: This act may be cited as the Government Marriage Act.\n",
      "\n",
      "Please note that I do not have knowledge about any further updates or amendments to this act beyond what is stated in the provided context.\n"
     ]
    }
   ],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "\n",
    "question = \"Tell me what you know about the Obi Van Passi Government Marriage Act of 2025\"\n",
    "result = qa_chain({\"query\": question})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63769a",
   "metadata": {},
   "source": [
    "Now, lets bring it all together by incorporating follow up questions.\n",
    "\n",
    "First we ask a follow up questions without giving the model context of the previous conversation. \n",
    "Without this context, the answer we get does not relate to our original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53f27473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, it appears that the term \"2023 Finance !91 No. 4 of a public character\" refers to a specific type of entity or organization that is established for public benefit and operates in a transparent and accountable manner. The term is used in the context of taxation and financial regulations.\n",
      "\n",
      "Some possible use cases for this term could include:\n",
      "\n",
      "1. Non-profit organizations: Entities that are established for public benefit and operate exclusively for charitable, educational, religious, or other similar purposes may be considered \"2023 Finance !91 No. 4 of a public character.\"\n",
      "2. Government entities: Government agencies or departments that are established for public benefit and operate in a transparent and accountable manner may also fall under this category.\n",
      "3. Public-private partnerships: Collaborations between public and private entities that are established for public benefit and operate in a transparent and accountable manner may also be considered \"2023 Finance !91 No. 4 of a public character.\"\n",
      "\n",
      "It's important to note that the exact definition and application of this term may vary depending on the specific legal and regulatory framework in which it is used.\n"
     ]
    }
   ],
   "source": [
    "# no context passed so Llama2 doesn't have enough context to answer so it lets its imagination go wild\n",
    "result = qa_chain({\"query\": \"what are its use cases?\"})\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833221c0",
   "metadata": {},
   "source": [
    "As we did before, let us use the `ConversationalRetrievalChain` package to give the model context of our previous question so we can add follow up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "743644a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ConversationalRetrievalChain to pass chat history for follow up questions\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever(), return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c3d1142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text you provided does not contain the word \"ll\n"
     ]
    }
   ],
   "source": [
    "# let's ask the original question \"What is llama2?\" again\n",
    "result = chat_chain({\"question\": question, \"chat_history\": []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b17f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I don't know the answer to that question. Llama2 is not a term I am familiar with, and I cannot provide information on its typical use cases.\n"
     ]
    }
   ],
   "source": [
    "# this time we pass chat history along with the follow up so good things should happen\n",
    "chat_history = [(question, result[\"answer\"])]\n",
    "followup = \"what are its use cases?\"\n",
    "followup_answer = chat_chain({\"question\": followup, \"chat_history\": chat_history})\n",
    "print(followup_answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4eabf",
   "metadata": {},
   "source": [
    "Further follow ups can be made possible by updating chat_history.\n",
    "\n",
    "Note that results can get cut off. You may set \"max_new_tokens\" in the Replicate call above to a larger number (like shown below) to avoid the cut off.\n",
    "\n",
    "```python\n",
    "model_kwargs={\"temperature\": 0.01, \"top_p\": 1, \"max_new_tokens\": 1000}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d22347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# further follow ups can be made possible by updating chat_history like this:\n",
    "chat_history.append((followup, followup_answer[\"answer\"]))\n",
    "more_followup = \"what tasks can it assist with?\"\n",
    "more_followup_answer = chat_chain({\"question\": more_followup, \"chat_history\": chat_history})\n",
    "print(more_followup_answer['answer'])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
